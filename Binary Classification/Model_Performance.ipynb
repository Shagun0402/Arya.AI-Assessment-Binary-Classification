{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ba724c",
   "metadata": {},
   "source": [
    "###### Importing external libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6024e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score,log_loss\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca73a7b",
   "metadata": {},
   "source": [
    "###### Loading Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f8d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train data\n",
    "train_data= pd.read_csv('../Binary Classification/dataset/training_set.csv', index_col=0)\n",
    "\n",
    "# Loading test data\n",
    "test_data = pd.read_csv('../Binary Classification/dataset/test_set.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58caf41",
   "metadata": {},
   "source": [
    "###### Splitting the data into X and Y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed33668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X = train_data.drop(['Y'], axis=1)\n",
    "y = train_data['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2caa5",
   "metadata": {},
   "source": [
    "###### Splitting the data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "381d132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into Train -Test Test\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.2,stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a9b90",
   "metadata": {},
   "source": [
    "###### Seelcting features using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1768f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection using Random Forest Classifier\n",
    "\n",
    "classifier = RandomForestClassifier(100, max_depth=None, n_jobs=1)\n",
    "classifier.fit(X_train, y_train)\n",
    "feature_imp = classifier.feature_importances_\n",
    "\n",
    "# Feature ranking based on importances\n",
    "imp = sorted(zip(X.columns,feature_imp), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extracting top 40 features\n",
    "top_features = [x[0] for x in imp[:40]]\n",
    "\n",
    "#Selecting top features from data\n",
    "\n",
    "X_train_final = X_train[top_features]\n",
    "X_test_final = X_test[top_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84532633",
   "metadata": {},
   "source": [
    "###### Normalizing the data using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b2ba66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing Data using Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_final)\n",
    "\n",
    "# Transorming the data\n",
    "X_train_Final = pd.DataFrame(scaler.transform(X_train_final), columns=X_train_final.columns)\n",
    "X_test_Final = pd.DataFrame(scaler.transform(X_test_final), columns=X_test_final.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52359595",
   "metadata": {},
   "source": [
    "###### Using PrettyTable to print model performances\n",
    "In order to use prettytable, open anaconda prompt in administrator mode and- \n",
    "\n",
    "1. pip install prettytable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "355ee47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using table to print model performance\n",
    "table = PrettyTable()\n",
    "table.field_names = ['Model', 'Train Log-loss', 'Validation Log-Loss', 'Train AUC', 'Validation AUC']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46848827",
   "metadata": {},
   "source": [
    "### MODEL TRAINING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea066846",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Models\n",
    "\n",
    "# 1. Random Model\n",
    "y_train_probab = np.random.rand(len(X_train_Final))\n",
    "y_test_probab = np.random.rand(len(X_test_Final))\n",
    "\n",
    "table.add_row(['Random', log_loss(y_train, y_train_probab), log_loss(y_test,y_test_probab), roc_auc_score(y_train, y_train_probab), roc_auc_score(y_test, y_test_probab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cc7a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Logistic Regression\n",
    "\n",
    "cls = LogisticRegression(C=1, penalty='l2', max_iter=250, random_state=42)\n",
    "cls.fit(X_train_Final, y_train)\n",
    "\n",
    "y_train_pred = cls.predict(X_train_Final)\n",
    "y_train_probab = cls.predict_proba(X_train_Final)[:,1]\n",
    "\n",
    "y_test_pred = cls.predict(X_test_Final)\n",
    "y_test_probab = cls.predict_proba(X_test_Final)[:,1]\n",
    "\n",
    "table.add_row(['Logistic Regression', log_loss(y_train, y_train_probab), log_loss(y_test,y_test_probab), roc_auc_score(y_train, y_train_probab), roc_auc_score(y_test, y_test_probab)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cb23213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Naive Bayes\n",
    "\n",
    "cls = GaussianNB()\n",
    "cls.fit(X_train_Final, y_train)\n",
    "\n",
    "y_train_pred = cls.predict(X_train_Final)\n",
    "y_train_probab = cls.predict_proba(X_train_Final)[:,1]\n",
    "\n",
    "y_test_pred = cls.predict(X_test_Final)\n",
    "y_test_probab = cls.predict_proba(X_test_Final)[:,1]\n",
    "\n",
    "table.add_row(['Naive Bayes', log_loss(y_train, y_train_probab), log_loss(y_test,y_test_probab), roc_auc_score(y_train, y_train_probab), roc_auc_score(y_test, y_test_probab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26994844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shagu\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "# 4. Support Vector Machine\n",
    "\n",
    "cls = LinearSVC(penalty='l2', max_iter=250, random_state=42, tol=1e-5)\n",
    "cls.fit(X_train_final, y_train)\n",
    "\n",
    "y_train_pred = cls.predict(X_train_final)\n",
    "y_train_probab = cls._predict_proba_lr(X_train_final)[:,1]\n",
    "\n",
    "y_test_pred = cls.predict(X_test_final)\n",
    "y_test_probab = cls._predict_proba_lr(X_test_final)[:,1]\n",
    "\n",
    "table.add_row(['Support Vector Machine', log_loss(y_train, y_train_probab), log_loss(y_test,y_test_probab), roc_auc_score(y_train, y_train_probab), roc_auc_score(y_test, y_test_probab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f2d6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. K-Nearest Neighbors\n",
    "\n",
    "cls = KNeighborsClassifier()\n",
    "cls.fit(X_train_final, y_train)\n",
    "\n",
    "y_train_pred = cls.predict(X_train_final)\n",
    "y_train_probab = cls.predict_proba(X_train_final)[:,1]\n",
    "\n",
    "y_test_pred = cls.predict(X_test_final)\n",
    "y_test_probab = cls.predict_proba(X_test_final)[:,1]\n",
    "\n",
    "table.add_row(['K-Nearest Neighbors', log_loss(y_train, y_train_probab), log_loss(y_test,y_test_probab), roc_auc_score(y_train, y_train_probab), roc_auc_score(y_test, y_test_probab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "666cab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Decision Tree\n",
    "\n",
    "cls = DecisionTreeClassifier(criterion='gini', min_samples_split=3, random_state=42)\n",
    "cls.fit(X_train_Final, y_train)\n",
    "\n",
    "y_train_pred = cls.predict(X_train_Final)\n",
    "y_train_probab = cls.predict_proba(X_train_Final)[:,1]\n",
    "\n",
    "y_test_pred = cls.predict(X_test_Final)\n",
    "y_test_probab = cls.predict_proba(X_test_Final)[:,1]\n",
    "\n",
    "table.add_row(['Decision Tree', log_loss(y_train, y_train_probab), log_loss(y_test,y_test_probab), roc_auc_score(y_train, y_train_probab), roc_auc_score(y_test, y_test_probab)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2aac639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Random Forest\n",
    "\n",
    "cls = RandomForestClassifier(n_estimators=500,\n",
    "                             max_depth=None,\n",
    "                             min_samples_split=3,\n",
    "                             n_jobs=1,\n",
    "                             class_weight='balanced',\n",
    "                             random_state=42)\n",
    "cls.fit(X_train_Final, y_train)\n",
    "\n",
    "y_train_pred = cls.predict(X_train_Final)\n",
    "y_train_probab = cls.predict_proba(X_train_Final)[:,1]\n",
    "\n",
    "y_test_pred = cls.predict(X_test_Final)\n",
    "y_test_probab = cls.predict_proba(X_test_Final)[:,1]\n",
    "\n",
    "table.add_row(['Random Forest', log_loss(y_train, y_train_probab), log_loss(y_test,y_test_probab), roc_auc_score(y_train, y_train_probab), roc_auc_score(y_test, y_test_probab)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5db30df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. XGBoost\n",
    "\n",
    "cls = XGBClassifier(n_estimators=500,\n",
    "                    max_depth=5,\n",
    "                    learning_rate=0.15,\n",
    "                    colsample_bytree=1,\n",
    "                    subsample=1,\n",
    "                    reg_alpha=0.3,\n",
    "                    gamma=10,\n",
    "                    n_jobs=2,\n",
    "                    eval_metric='logloss',\n",
    "                    use_label_encoder=False)\n",
    "\n",
    "cls.fit(X_train_Final, y_train)\n",
    "\n",
    "y_train_pred = cls.predict(X_train_Final)\n",
    "y_train_probab = cls.predict_proba(X_train_Final)[:,1]\n",
    "\n",
    "y_test_pred = cls.predict(X_test_Final)\n",
    "y_test_probab = cls.predict_proba(X_test_Final)[:,1]\n",
    "\n",
    "table.add_row(['XG Boost', log_loss(y_train, y_train_probab), log_loss(y_test,y_test_probab), roc_auc_score(y_train, y_train_probab), roc_auc_score(y_test, y_test_probab)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8845eece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------------------+---------------------+---------------------+--------------------+\n",
      "|         Model          |    Train Log-loss    | Validation Log-Loss |      Train AUC      |   Validation AUC   |\n",
      "+------------------------+----------------------+---------------------+---------------------+--------------------+\n",
      "|         Random         |  1.0144442347521994  |  0.9942432390814736 | 0.49277414580838724 | 0.5141162352134407 |\n",
      "|  Logistic Regression   | 0.20343928905281705  | 0.31541725743815563 |  0.975182280848196  | 0.9613029315960911 |\n",
      "|      Naive Bayes       |  2.0139335434506673  |  2.521361516681172  |  0.9553053405169586 | 0.9353334476255786 |\n",
      "| Support Vector Machine |  0.5782344521164527  |  0.5679838520005609 |  0.8519897090151582 | 0.8512669295388308 |\n",
      "|  K-Nearest Neighbors   | 0.26583437688099887  |  1.6390116128831518 |  0.9478734865662863 |  0.86351105777473  |\n",
      "|     Decision Tree      | 0.005761453546854756 |  3.5811343475729838 |  0.999963773195337  | 0.8917160980627465 |\n",
      "|     Random Forest      | 0.051025886003277274 | 0.14693115307184293 |  0.9999987138412546 | 0.9929915995199726 |\n",
      "|        XG Boost        | 0.14191481391446728  |  0.1540978423955724 |  0.9894479249329161 | 0.988321618378193  |\n",
      "+------------------------+----------------------+---------------------+---------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2aeadc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
