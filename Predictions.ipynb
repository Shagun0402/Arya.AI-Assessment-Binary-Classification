{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8883b44d",
   "metadata": {},
   "source": [
    "###### Importing External Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9098a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report,confusion_matrix,log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97642583",
   "metadata": {},
   "source": [
    "###### Loading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e597cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (3910, 58)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>X51</th>\n",
       "      <th>X52</th>\n",
       "      <th>X53</th>\n",
       "      <th>X54</th>\n",
       "      <th>X55</th>\n",
       "      <th>X56</th>\n",
       "      <th>X57</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.200</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>16.304</td>\n",
       "      <td>148</td>\n",
       "      <td>375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.440</td>\n",
       "      <td>22</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.46</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.338</td>\n",
       "      <td>123</td>\n",
       "      <td>207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1    X2    X3     X4    X5    X6    X7    X8   X9   X10  ...  X49  \\\n",
       "0  0.00  0.00  4.34   0.00  0.00  0.00  0.00  0.00  0.0  0.00  ...  0.0   \n",
       "1  0.00  0.56  0.56   0.00  1.12  0.56  2.25  0.00  0.0  0.56  ...  0.0   \n",
       "2  0.00  0.00  0.00   0.00  0.00  0.00  0.00  0.00  0.0  0.00  ...  0.0   \n",
       "3  0.64  0.00  0.64   0.00  1.93  0.00  0.00  0.00  0.0  0.00  ...  0.0   \n",
       "4  0.58  0.00  0.00  35.46  0.58  0.00  0.58  0.58  0.0  0.00  ...  0.0   \n",
       "\n",
       "     X50  X51    X52    X53    X54     X55  X56  X57  Y  \n",
       "0  0.000  0.0  1.342  0.000  0.000   1.200    2   12  0  \n",
       "1  0.083  0.0  0.503  0.000  0.083  16.304  148  375  1  \n",
       "2  0.000  0.0  0.000  0.000  0.000   1.000    1    5  0  \n",
       "3  0.000  0.0  0.462  0.370  0.000   2.440   22  122  1  \n",
       "4  0.000  0.0  0.239  0.239  0.000   3.338  123  207  1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Train data\n",
    "train_data = pd.read_csv('../Binary Classification/dataset/training_set.csv', index_col=0)\n",
    "print('Train data shape: ',train_data.shape)\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6de4e0",
   "metadata": {},
   "source": [
    "###### Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b118094f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (691, 57)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X48</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>X51</th>\n",
       "      <th>X52</th>\n",
       "      <th>X53</th>\n",
       "      <th>X54</th>\n",
       "      <th>X55</th>\n",
       "      <th>X56</th>\n",
       "      <th>X57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.342</td>\n",
       "      <td>47</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.375</td>\n",
       "      <td>168</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.153</td>\n",
       "      <td>5.891</td>\n",
       "      <td>193</td>\n",
       "      <td>3040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.015</td>\n",
       "      <td>8.550</td>\n",
       "      <td>669</td>\n",
       "      <td>1351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.781</td>\n",
       "      <td>32</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1   X2    X3   X4    X5    X6    X7    X8    X9   X10  ...  X48  X49  \\\n",
       "0  0.70  0.0  0.70  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.0  0.0   \n",
       "1  0.00  0.0  0.84  0.0  0.84  0.00  0.84  0.00  0.00  0.00  ...  0.0  0.0   \n",
       "2  0.46  0.3  0.46  0.0  0.05  0.12  0.05  0.28  0.43  0.74  ...  0.0  0.0   \n",
       "3  0.10  0.2  1.01  0.0  0.80  0.80  0.50  0.00  0.80  0.10  ...  0.0  0.0   \n",
       "4  0.00  0.0  0.72  0.0  0.72  0.00  0.72  0.00  0.00  0.00  ...  0.0  0.0   \n",
       "\n",
       "     X50  X51    X52    X53    X54     X55  X56   X57  \n",
       "0  0.000  0.0  0.105  0.000  0.000   2.342   47    89  \n",
       "1  0.388  0.0  0.776  0.129  0.000  10.375  168   249  \n",
       "2  0.065  0.0  0.325  0.756  0.153   5.891  193  3040  \n",
       "3  0.110  0.0  0.490  0.158  0.015   8.550  669  1351  \n",
       "4  0.364  0.0  0.729  0.121  0.000   7.781   32   249  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Test Data\n",
    "\n",
    "test_data = pd.read_csv('../Binary Classification/dataset/test_set.csv', index_col=0)\n",
    "print('Test data shape:', test_data.shape)\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ffd82",
   "metadata": {},
   "source": [
    "###### Defining the exploratory features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8729ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(['Y'], axis=1)\n",
    "y = train_data['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a17fd1",
   "metadata": {},
   "source": [
    "###### Based on the problem statement, We will split the data into train and validation set using 80/20 ratios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39aa88f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d98bf7",
   "metadata": {},
   "source": [
    "###### Feature Selection based on Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1afa827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Selection using RandomForestClassifer\n",
    "\n",
    "classifier = RandomForestClassifier(100, max_depth=None, n_jobs=1)\n",
    "classifier.fit(X_train, y_train)\n",
    "feature_imp = classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e42befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking features based on importance\n",
    "importances = sorted(zip(X.columns, feature_imp), key=lambda x: x[1], reverse=True)\n",
    "top_features= [x[0] for x in importances[:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a9e078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting top features\n",
    "X_train_final = X_train[top_features]\n",
    "X_test_final = X_test[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b721040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting top features in test data\n",
    "test_data_final = test_data[top_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4afa535",
   "metadata": {},
   "source": [
    "###### Based on pervious analysis, The selected model is XGBoost Classifier\n",
    "##### Training the model on train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c7f54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model Training\n",
    "\n",
    "cls = XGBClassifier(n_estimators=500,\n",
    "                    max_depth=5,\n",
    "                    learning_rate=0.15,\n",
    "                    colsample_bytree=1,\n",
    "                    subsample=1,\n",
    "                    reg_alpha=0.3,\n",
    "                    gamma=10,\n",
    "                    n_jobs=2,\n",
    "                    eval_metric='logloss',\n",
    "                    use_label_encoder=False)\n",
    "\n",
    "cls.fit(X_train_final, y_train)\n",
    "\n",
    "y_train_pred = cls.predict(X_train_final)\n",
    "y_train_probab = cls.predict_proba(X_train_final)[:,1]\n",
    "\n",
    "y_test_pred = cls.predict(X_test_final)\n",
    "y_test_probab = cls.predict_proba(X_test_final)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b14b738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Log-loss: 0.14277005066191228\n",
      "Validation Log-loss 0.15530222387927228\n",
      "\n",
      "\n",
      "Train AUC Score: 0.9894357064248345\n",
      "Validation AUC Score: 0.9879204525972913\n"
     ]
    }
   ],
   "source": [
    "## Logloss\n",
    "print('Train Log-loss:' , log_loss(y_train, y_train_probab))\n",
    "print('Validation Log-loss', log_loss(y_test,y_test_probab))\n",
    "\n",
    "## AUC Score\n",
    "print('\\n')\n",
    "print('Train AUC Score:', roc_auc_score(y_train, y_train_probab))\n",
    "print('Validation AUC Score:', roc_auc_score(y_test, y_test_probab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6649ab",
   "metadata": {},
   "source": [
    "###### Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09a5639c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1\n",
      " 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 0\n",
      " 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0\n",
      " 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1\n",
      " 1 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1\n",
      " 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1\n",
      " 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0\n",
      " 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0\n",
      " 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1\n",
      " 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1\n",
      " 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Predicting on Test Data\n",
    "\n",
    "test_pred = cls.predict(test_data_final)\n",
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "375d268a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04033171 0.97785157 0.98079556 0.9691561  0.97785157 0.20852894\n",
      " 0.97279274 0.73098963 0.95876235 0.9805946  0.01199517 0.06750625\n",
      " 0.22864741 0.01544631 0.07676392 0.11900955 0.02213419 0.13877682\n",
      " 0.96227586 0.02222986 0.9739934  0.97767943 0.9691878  0.01279566\n",
      " 0.73592615 0.07698447 0.13139409 0.85809475 0.07698447 0.01394515\n",
      " 0.9033491  0.05301218 0.01544631 0.04568623 0.01479496 0.04990736\n",
      " 0.4979711  0.03954214 0.5295856  0.49401128 0.98281235 0.10667392\n",
      " 0.13877682 0.09131541 0.04722621 0.9440237  0.08039155 0.0202093\n",
      " 0.03212925 0.9776448  0.88717324 0.15313905 0.0133681  0.9235822\n",
      " 0.975184   0.97291476 0.02346839 0.02158874 0.37456363 0.9581312\n",
      " 0.97917295 0.1366199  0.33742198 0.24190742 0.29049578 0.9568\n",
      " 0.9196574  0.7921646  0.12864298 0.04568623 0.02074446 0.8677428\n",
      " 0.04568623 0.57246137 0.93341833 0.9678928  0.09657373 0.01244521\n",
      " 0.33559394 0.04568623 0.07504966 0.94459915 0.929011   0.96571946\n",
      " 0.04568623 0.01544631 0.08297646 0.04568623 0.96464837 0.01668686\n",
      " 0.05301218 0.12019876 0.02312122 0.10542703 0.03900386 0.8019395\n",
      " 0.6611286  0.11209705 0.9743726  0.77841073 0.01199517 0.02887985\n",
      " 0.24442913 0.75032556 0.9733752  0.0407466  0.2252208  0.04052372\n",
      " 0.04717787 0.9665109  0.11257859 0.91324776 0.04286082 0.9710514\n",
      " 0.9787467  0.04390347 0.2252208  0.05301218 0.04795582 0.02834275\n",
      " 0.2549564  0.02694561 0.03980017 0.16482091 0.08371749 0.03920634\n",
      " 0.0416222  0.65396726 0.01544631 0.97569865 0.53616494 0.05502472\n",
      " 0.05834614 0.04864935 0.97377926 0.04568623 0.06828831 0.6393901\n",
      " 0.06128532 0.9754438  0.07698447 0.02939571 0.06506392 0.03021188\n",
      " 0.01475882 0.9526486  0.9403125  0.23533277 0.75880545 0.95770663\n",
      " 0.01437752 0.9623697  0.97472465 0.03515216 0.05193977 0.03164849\n",
      " 0.9713118  0.04568623 0.01141071 0.43437967 0.97896683 0.0493385\n",
      " 0.01544631 0.031135   0.8492495  0.03295592 0.04539564 0.05301218\n",
      " 0.10171764 0.01381538 0.03371742 0.98095477 0.26411822 0.98079556\n",
      " 0.02411496 0.12425615 0.97904986 0.04568623 0.9677223  0.9277928\n",
      " 0.03487657 0.0366072  0.01155484 0.9691561  0.12266084 0.09266933\n",
      " 0.94452894 0.2617299  0.0668838  0.98021746 0.81813097 0.05443693\n",
      " 0.9691561  0.6096168  0.74598587 0.8475786  0.07696451 0.05301218\n",
      " 0.19895451 0.97126365 0.04568623 0.94777524 0.07707829 0.01182317\n",
      " 0.102401   0.9505368  0.8770089  0.04717787 0.9806113  0.0986318\n",
      " 0.36458215 0.95652264 0.0263276  0.01666442 0.03188926 0.12250566\n",
      " 0.06806298 0.12425615 0.01475882 0.9779333  0.02417063 0.9663332\n",
      " 0.02976409 0.18082866 0.05301218 0.82252467 0.9598221  0.04568623\n",
      " 0.04568623 0.01141071 0.10576315 0.02765086 0.906949   0.01175972\n",
      " 0.05301218 0.22188516 0.97084683 0.03807681 0.05318696 0.01168443\n",
      " 0.04760147 0.06422535 0.03878047 0.01952567 0.13518168 0.07804541\n",
      " 0.06709827 0.02392414 0.0515213  0.9733166  0.07698447 0.9592127\n",
      " 0.05771336 0.86155075 0.02487518 0.03188926 0.58167505 0.6314895\n",
      " 0.9671718  0.96504617 0.02355698 0.01544631 0.9507612  0.04568623\n",
      " 0.96227586 0.03711156 0.04568623 0.13877682 0.7247758  0.03643421\n",
      " 0.01544631 0.9661664  0.98079556 0.04565248 0.972009   0.7038631\n",
      " 0.56303304 0.06133636 0.01805821 0.01199517 0.01431509 0.01544631\n",
      " 0.8746815  0.12944521 0.8956111  0.11406418 0.04568623 0.930821\n",
      " 0.27390605 0.05764829 0.5562459  0.03544569 0.03142645 0.88717324\n",
      " 0.84216857 0.01141071 0.42089307 0.04444232 0.04568623 0.96846753\n",
      " 0.01581559 0.9675312  0.01141071 0.02634394 0.26054522 0.02591243\n",
      " 0.02188949 0.98076725 0.02173415 0.956628   0.66404927 0.9584424\n",
      " 0.05712447 0.0335452  0.85750806 0.03748831 0.02200692 0.09634939\n",
      " 0.0333603  0.02899065 0.09875245 0.13877682 0.31444758 0.9730453\n",
      " 0.01544631 0.89350766 0.02566327 0.9691561  0.03722294 0.01544631\n",
      " 0.02411496 0.7698902  0.8081234  0.05301218 0.61310506 0.7136357\n",
      " 0.08643748 0.22623597 0.06335917 0.08346941 0.24442913 0.02733472\n",
      " 0.04568623 0.9723493  0.05316932 0.05301218 0.67750883 0.02118866\n",
      " 0.14720942 0.4491856  0.03142645 0.9671718  0.06642805 0.01175972\n",
      " 0.9653854  0.02118866 0.02118866 0.08346941 0.49323916 0.34947005\n",
      " 0.04568623 0.02757926 0.04907435 0.059453   0.05434949 0.03793907\n",
      " 0.96598244 0.02118866 0.6834325  0.9428455  0.933449   0.9470839\n",
      " 0.08294738 0.01337292 0.7136594  0.02783038 0.09290304 0.9395615\n",
      " 0.98025644 0.9649     0.97291476 0.97583187 0.0318638  0.06734671\n",
      " 0.01544631 0.02593903 0.9735246  0.04568623 0.0231591  0.9779333\n",
      " 0.8838542  0.98092455 0.38943192 0.97353697 0.9725729  0.9785338\n",
      " 0.05112869 0.07101326 0.01544631 0.958099   0.91439414 0.97431415\n",
      " 0.9537238  0.9464745  0.9681637  0.9386109  0.07784119 0.9746611\n",
      " 0.04808886 0.0255015  0.02213419 0.9785018  0.9801721  0.030216\n",
      " 0.03589499 0.04055654 0.02158874 0.03445439 0.04568623 0.97328454\n",
      " 0.9626504  0.92336625 0.06642805 0.24442913 0.93451244 0.09902608\n",
      " 0.04135157 0.1423023  0.40326473 0.10158151 0.9537238  0.33649307\n",
      " 0.02475974 0.18268995 0.09186115 0.02931551 0.05032225 0.9791967\n",
      " 0.97742915 0.1933371  0.27086213 0.9701286  0.96802306 0.22913408\n",
      " 0.03722294 0.07681231 0.02038408 0.98281235 0.94585687 0.20617004\n",
      " 0.59089804 0.93318605 0.9650011  0.03971044 0.07576918 0.9594767\n",
      " 0.07786591 0.24773008 0.9584679  0.95235074 0.01199517 0.07698447\n",
      " 0.9330933  0.82252467 0.02118866 0.9255012  0.08643575 0.01544631\n",
      " 0.96350193 0.9241874  0.0138607  0.27736944 0.9730453  0.04722621\n",
      " 0.26068288 0.94257104 0.9783572  0.93736494 0.9772668  0.9148086\n",
      " 0.01544631 0.0580432  0.9799461  0.02101703 0.06797363 0.1969181\n",
      " 0.01977182 0.13853    0.02118866 0.04568623 0.0260674  0.93984246\n",
      " 0.0335452  0.26457632 0.9081094  0.9569659  0.937363   0.4150221\n",
      " 0.93668103 0.06881096 0.43077973 0.96059686 0.09709208 0.06757507\n",
      " 0.96964    0.34992665 0.8808213  0.04568623 0.9811002  0.5729393\n",
      " 0.22231494 0.60233444 0.01544631 0.9462431  0.04568623 0.19496271\n",
      " 0.0123068  0.96357095 0.01850111 0.0408676  0.01550948 0.01092116\n",
      " 0.96273494 0.5733593  0.02157911 0.02387364 0.01377614 0.02572048\n",
      " 0.884038   0.04568623 0.18485934 0.21417142 0.04697597 0.02069696\n",
      " 0.04568623 0.97422564 0.09279402 0.02237796 0.03763609 0.01216223\n",
      " 0.07698447 0.04370074 0.0117076  0.07337493 0.06095235 0.02686744\n",
      " 0.12167966 0.01737898 0.0282325  0.04568623 0.08998113 0.16719769\n",
      " 0.940419   0.01544631 0.02118866 0.07698447 0.93341833 0.09145244\n",
      " 0.7804985  0.94205564 0.14306758 0.9678928  0.14250588 0.08481786\n",
      " 0.10507419 0.89345217 0.02312122 0.04568623 0.04568623 0.7449186\n",
      " 0.01544631 0.86370003 0.7553812  0.97453225 0.01199517 0.03195777\n",
      " 0.27811024 0.01544631 0.01723391 0.01544631 0.96644676 0.9710514\n",
      " 0.8408983  0.55222005 0.27578533 0.01544631 0.97547114 0.95552516\n",
      " 0.0411755  0.22910325 0.9663603  0.27789113 0.89291054 0.9626004\n",
      " 0.9470839  0.9787048  0.03856306 0.92108375 0.01923433 0.01544631\n",
      " 0.01219354 0.8686423  0.9625898  0.02118866 0.9787243  0.02436333\n",
      " 0.04773626 0.06301374 0.01175972 0.95578885 0.9462671  0.9413121\n",
      " 0.04568623 0.01583862 0.12368576 0.04331959 0.9489455  0.9483323\n",
      " 0.02411496 0.96644676 0.9717324  0.059453   0.07098903 0.01350474\n",
      " 0.9531227  0.97202355 0.92361444 0.01167054 0.8602003  0.42621168\n",
      " 0.0742088  0.04738656 0.9442679  0.01168443 0.04017236 0.9805048\n",
      " 0.02828186 0.8080637  0.948541   0.01455647 0.9507612  0.01350474\n",
      " 0.5041194  0.0668838  0.9763174  0.03473483 0.01892526 0.07504966\n",
      " 0.9808621  0.06299023 0.9417966  0.01475882 0.05617435 0.03156516\n",
      " 0.22693896 0.04282115 0.89472103 0.961982   0.854289   0.9806113\n",
      " 0.97503847 0.88973296 0.94885737 0.9649948  0.04568623 0.0216005\n",
      " 0.29429924 0.13591434 0.95320934 0.9239274  0.7804985  0.9590681\n",
      " 0.97117615 0.05341888 0.93529797 0.5976564  0.02639585 0.5921939\n",
      " 0.02809819 0.10420865 0.07101326 0.86913776 0.9570009  0.01168443\n",
      " 0.97169095 0.01227525 0.02455244 0.01859592 0.01983948 0.03899194\n",
      " 0.03722294]\n"
     ]
    }
   ],
   "source": [
    "test_probab= cls.predict_proba(test_data_final)[:,1]\n",
    "print(test_probab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a08d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
